## ğŸ“˜ Symbolic Regression: Architectural Progress & Observations

### ğŸ” True Function Example 1

```
y = 0.8 * (0.3 * x1 + 1.4 * x2 + 0.7 * x3 + 0.6)^0.5
```

#### ğŸ”¹ Mixed Model (Shallow Architecture)

```
Epoch 11200, Loss: 0.000005  Test Loss: 0.000005
Learned symbolic expression:
0.745 * (x1^-0.014 * x2^0.153 * x3^-0.001) + (0.066*x1 + 0.192*x2 + 0.138*x3 + 0.257)
```

- â— Despite low loss, the symbolic form is **not aligned** with the true structure.
- **Interpretation**: The model fails to discover the inner linear form due to limited depth.

#### ğŸ”¹ Sparse Pruned Model (Deeper Compositional Structure)

```
Simplified symbolic expression:
y = 0.892 * ((0.210*x1 + 0.981*x2 + 0.490*x3 + 0.610))^0.533  (i.e., y3)
```

- âœ… Much closer to ground truth â€” **captures the linear transform and exponentiation**.
- **Conclusion**: Making the network **deeper and modular** helps uncover latent structure.

---

### ğŸ” True Function Example 2

```
y = 0.4 * (x1^0.3 * x2^0.7 * x3^0.6) + 0.3
```

#### ğŸ”¹ Sparse Pruned Model

```
Simplified symbolic expression:
y = 0.386 * (x1^0.306 * x2^0.715 * x3^0.613) + 0.316  (i.e., y4)
```

- âœ… Precisely identifies **product-based structure** and accurate exponents.
- **Conclusion**: The model generalizes well across different structural forms.

---

### ğŸ§  Expression Form Learned by Sparse Model (Symbolically)

We assume:

- First-layer outputs:
  - Power: yâ‚ = Câ‚ * âˆ xáµ¢^wáµ¢
  - Linear: yâ‚‚ = âˆ‘ aáµ¢ * xáµ¢ + bâ‚
- Second-layer operations:
  - Power activation: yâ‚ƒ = Câ‚‚ * yâ‚^wâ‚‚
  - Linear transformation: yâ‚„ = aâ‚‚ * yâ‚‚ + bâ‚‚

Combined:

```
y = Câ‚‚ * (Câ‚ * âˆ xáµ¢^wáµ¢)^wâ‚‚ + aâ‚‚ * (âˆ‘ aáµ¢ * xáµ¢ + bâ‚) + bâ‚‚
```

Simplified:

```
y = [Câ‚‚ * Câ‚^wâ‚‚] * âˆ xáµ¢^(wáµ¢ * wâ‚‚) + aâ‚‚ * âˆ‘ aáµ¢ * xáµ¢ + aâ‚‚ * bâ‚ + bâ‚‚
```

---

### âš ï¸ Challenges & Insights

- **Two Key Tasks**:
  1. Identify the structure of the function (e.g., product vs. sum).
  2. Learn accurate coefficients and exponents.

- **Parameter Initialization Matters**:
  - If the true parameter is outside the initialized range, it becomes hard to reach.
  - Gradient-based learning is sensitive to **parameter scale imbalance**.
  - Especially critical for:
    - Exponents `wáµ¢`
    - Linear weights `aáµ¢`
    - Scalars `Câ‚`, `Câ‚‚`, etc.

- **Structural Ambiguity**:
  - Different expressions can represent the same function value.
  - This makes optimization harder without proper architectural priors.

---

### ğŸ”§ Design Consideration

- **Positive Intermediate Values**:
  - All intermediate activations are constrained to be positive.
  - Alternative: use `F.softplus()` to allow more flexibility while staying positive.